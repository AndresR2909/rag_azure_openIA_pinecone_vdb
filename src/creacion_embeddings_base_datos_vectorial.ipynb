{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1537e6",
   "metadata": {},
   "source": [
    "# Preparación y representación vectorial de documentos/textos en embeddings, indexación y almacenamiento en una base de datos vectorial\n",
    "\n",
    "referencias:\n",
    "\n",
    "[1] https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/README.md\n",
    "\n",
    "[2]https://github.com/pinecone-io/examples/blob/master/learn/generation/openai/gpt-4-langchain-docs.ipynb\n",
    "\n",
    "\n",
    "[3] https://docs.pinecone.io/docs/quickstart\n",
    "\n",
    "\n",
    "[4]https://github.com/Azure-Samples/Azure-OpenAI-Docs-Samples/blob/main/Samples/Tutorials/Embeddings/embedding_billsum.ipynb\n",
    "\n",
    "Este notebook toma los ejemplos de la referencia 2 y 4 y los aplica al dataset de tweets de cambio climatico:\n",
    "\n",
    "* Leer conjunto de datos y preprocesarlo.\n",
    "* Limpiar el texto y/o documento y crear embeddings (representacion vectorial) con modelo seleccionado: openIA ada-002 \n",
    "* Realizar una busqueda de texto a traves de una seleccion de vectores similares en el dataframe.\n",
    "* Crear el indice y agregar los embeddings en el indice creado de la base de datos vectorial seleccionada: pinecone.\n",
    "\n",
    "\n",
    "### Base de datos vectorial seleccionada pinecone\n",
    "\n",
    "- **configuracion**: Instalar, Importar librerias y cargar variables de ambiente para conexion a base de datos vectorial pinecone y a modelo de embeddings openIA. [2][3][4]\n",
    "- **leer dataset y crear embdedings**: leer el dataset, aumentar el texto y crear embeddings con modelo de OpenIA.\n",
    "- **Base de datos vectorial: Pinecone**\n",
    "    - Configurar y crear el cliente para conexion con VDB Pinecone. \n",
    "    - Crear indice\n",
    "    - Cargar vectores y metadata en indice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eeaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai python-dotenv pinecone-client numpy pandas tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b59250",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0455ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "from pinecone import Pinecone\n",
    "from huggingface_hub import login\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43297f80",
   "metadata": {},
   "source": [
    "## configurar variables de ambiente y rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8753d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "OPENAI_API_KEY = os.environ[\"AZURE_OPENAI_API_KEY\"] \n",
    "EMBEDDING_DEPLOYMENT = os.environ[\"AZURE_EMBEDDING_DEPLOYMENT\"] \n",
    "OPENAI_API_VERSION =os.environ[\"OPENAI_API_VERSION\"] \n",
    "\n",
    "api_key = os.environ.get('PINECONE_API_KEY')\n",
    "environment = os.environ.get('PINECONE_ENVIRONMENT')\n",
    "use_serverless = os.environ.get(\"USE_SERVERLESS\", \"False\").lower() == \"true\"\n",
    "access_token_hf = os.environ.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563013bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = os.getcwd()\n",
    "data_path = '../data'\n",
    "filename = 'climateTwitterData.csv'\n",
    "out_data_path = data_path+'data/out/batch/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4f970",
   "metadata": {},
   "source": [
    "## Leer dataset de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1832f9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_2468\\788048299.py:1: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(os.path.join(src_path,data_path,filename))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>urls</th>\n",
       "      <th>search_hashtags</th>\n",
       "      <th>location</th>\n",
       "      <th>sentiment1</th>\n",
       "      <th>sentiment2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.211810e+18</td>\n",
       "      <td>7.590000e+17</td>\n",
       "      <td>2020 is the year we #votethemout, the year we ...</td>\n",
       "      <td>15</td>\n",
       "      <td>https://twitter.com/Sphiamia/status/1211807074...</td>\n",
       "      <td>2019-12-31 00:31:35+00:00</td>\n",
       "      <td>Tue Dec 31 00:31:35 +0000 2019</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#votethemout #climatestrike #rebelforlife</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.210670e+18</td>\n",
       "      <td>2.219547e+07</td>\n",
       "      <td>Winter has not stopped this group of dedicated...</td>\n",
       "      <td>9</td>\n",
       "      <td>https://twitter.com/StephDujarric/status/12106...</td>\n",
       "      <td>2019-12-27 20:56:21+00:00</td>\n",
       "      <td>Fri Dec 27 20:56:21 +0000 2019</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#climatefriday #climatestrike #ClimateAction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.210590e+18</td>\n",
       "      <td>1.070000e+18</td>\n",
       "      <td>WEEK 55 of #ClimateStrike at the @UN. Next wee...</td>\n",
       "      <td>545</td>\n",
       "      <td>https://twitter.com/AlexandriaV2005/status/121...</td>\n",
       "      <td>2019-12-27 15:50:22+00:00</td>\n",
       "      <td>Fri Dec 27 15:50:22 +0000 2019</td>\n",
       "      <td>3283</td>\n",
       "      <td>@UN @Fridays4future</td>\n",
       "      <td>#ClimateStrike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.210260e+18</td>\n",
       "      <td>1.339821e+09</td>\n",
       "      <td>A year of resistance, as youth protests shape...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/EnergyHouseVA/status/12102...</td>\n",
       "      <td>2019-12-26 17:53:26+00:00</td>\n",
       "      <td>Thu Dec 26 17:53:26 +0000 2019</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#greta #gretathunberg #climatechange #fridaysf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.channelnewsasia.com/news/commentar...</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.209640e+18</td>\n",
       "      <td>1.339821e+09</td>\n",
       "      <td>HAPPY HOLIDAYS #greta #gretathunberg #climate...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/EnergyHouseVA/status/12096...</td>\n",
       "      <td>2019-12-25 00:56:37+00:00</td>\n",
       "      <td>Wed Dec 25 00:56:37 +0000 2019</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#greta #gretathunberg #climatechange #fridaysf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.energyhouse.us,http://www.pacenowfo...</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0            id     author_id  \\\n",
       "0             0           0  1.211810e+18  7.590000e+17   \n",
       "1             1           1  1.210670e+18  2.219547e+07   \n",
       "2             2           2  1.210590e+18  1.070000e+18   \n",
       "3             3           3  1.210260e+18  1.339821e+09   \n",
       "4             4           4  1.209640e+18  1.339821e+09   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  2020 is the year we #votethemout, the year we ...        15   \n",
       "1  Winter has not stopped this group of dedicated...         9   \n",
       "2  WEEK 55 of #ClimateStrike at the @UN. Next wee...       545   \n",
       "3   A year of resistance, as youth protests shape...         1   \n",
       "4   HAPPY HOLIDAYS #greta #gretathunberg #climate...         1   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/Sphiamia/status/1211807074...   \n",
       "1  https://twitter.com/StephDujarric/status/12106...   \n",
       "2  https://twitter.com/AlexandriaV2005/status/121...   \n",
       "3  https://twitter.com/EnergyHouseVA/status/12102...   \n",
       "4  https://twitter.com/EnergyHouseVA/status/12096...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-12-31 00:31:35+00:00  Tue Dec 31 00:31:35 +0000 2019         46   \n",
       "1  2019-12-27 20:56:21+00:00  Fri Dec 27 20:56:21 +0000 2019         35   \n",
       "2  2019-12-27 15:50:22+00:00  Fri Dec 27 15:50:22 +0000 2019       3283   \n",
       "3  2019-12-26 17:53:26+00:00  Thu Dec 26 17:53:26 +0000 2019          2   \n",
       "4  2019-12-25 00:56:37+00:00  Wed Dec 25 00:56:37 +0000 2019          4   \n",
       "\n",
       "              mentions                                           hashtags  \\\n",
       "0                  NaN          #votethemout #climatestrike #rebelforlife   \n",
       "1                  NaN       #climatefriday #climatestrike #ClimateAction   \n",
       "2  @UN @Fridays4future                                     #ClimateStrike   \n",
       "3                  NaN  #greta #gretathunberg #climatechange #fridaysf...   \n",
       "4                  NaN  #greta #gretathunberg #climatechange #fridaysf...   \n",
       "\n",
       "   geo                                               urls search_hashtags  \\\n",
       "0  NaN                                                NaN  #climatestrike   \n",
       "1  NaN                                                NaN  #climatestrike   \n",
       "2  NaN                                                NaN  #climatestrike   \n",
       "3  NaN  https://www.channelnewsasia.com/news/commentar...  #climatestrike   \n",
       "4  NaN  http://www.energyhouse.us,http://www.pacenowfo...  #climatestrike   \n",
       "\n",
       "          location sentiment1 sentiment2  \n",
       "0  California, USA   negative   negative  \n",
       "1  California, USA   positive   positive  \n",
       "2  California, USA   positive   positive  \n",
       "3  California, USA   positive   positive  \n",
       "4  California, USA   positive   positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(os.path.join(src_path,data_path,filename))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a335d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search_hashtags\n",
       "#climatestrike       18355\n",
       "#climatechange       16190\n",
       "#climateaction        6378\n",
       "#sustainability       5790\n",
       "#climatecrisis        4982\n",
       "#environment          4703\n",
       "#greennewdeal         4589\n",
       "#globalwarming        4152\n",
       "#fridaysforfuture     3038\n",
       "#actonclimate         1895\n",
       "#savetheplanet        1434\n",
       "#bushfires             899\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['search_hashtags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66e9f126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72405 entries, 0 to 72404\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0.1     72405 non-null  int64  \n",
      " 1   Unnamed: 0       72405 non-null  int64  \n",
      " 2   id               72405 non-null  float64\n",
      " 3   author_id        72405 non-null  float64\n",
      " 4   text             72405 non-null  object \n",
      " 5   retweets         72405 non-null  int64  \n",
      " 6   permalink        72405 non-null  object \n",
      " 7   date             72405 non-null  object \n",
      " 8   formatted_date   72405 non-null  object \n",
      " 9   favorites        72405 non-null  int64  \n",
      " 10  mentions         27554 non-null  object \n",
      " 11  hashtags         72402 non-null  object \n",
      " 12  geo              0 non-null      float64\n",
      " 13  urls             33349 non-null  object \n",
      " 14  search_hashtags  72405 non-null  object \n",
      " 15  location         72405 non-null  object \n",
      " 16  sentiment1       30000 non-null  object \n",
      " 17  sentiment2       30000 non-null  object \n",
      "dtypes: float64(3), int64(4), object(11)\n",
      "memory usage: 9.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e44e0",
   "metadata": {},
   "source": [
    "## Preprocesar dataset para crear embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b43060",
   "metadata": {},
   "source": [
    "### pre-pocesar dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7c4e5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>search_hashtags</th>\n",
       "      <th>location</th>\n",
       "      <th>sentiment1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020 is the year we #votethemout, the year we ...</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>#votethemout #climatestrike #rebelforlife</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Winter has not stopped this group of dedicated...</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>#climatefriday #climatestrike #ClimateAction</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WEEK 55 of #ClimateStrike at the @UN. Next wee...</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>#ClimateStrike</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A year of resistance, as youth protests shape...</td>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>#greta #gretathunberg #climatechange #fridaysf...</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAPPY HOLIDAYS #greta #gretathunberg #climate...</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>#greta #gretathunberg #climatechange #fridaysf...</td>\n",
       "      <td>#climatestrike</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  2020 is the year we #votethemout, the year we ...  2019-12-31   \n",
       "1  Winter has not stopped this group of dedicated...  2019-12-27   \n",
       "2  WEEK 55 of #ClimateStrike at the @UN. Next wee...  2019-12-27   \n",
       "3   A year of resistance, as youth protests shape...  2019-12-26   \n",
       "4   HAPPY HOLIDAYS #greta #gretathunberg #climate...  2019-12-25   \n",
       "\n",
       "                                            hashtags search_hashtags  \\\n",
       "0          #votethemout #climatestrike #rebelforlife  #climatestrike   \n",
       "1       #climatefriday #climatestrike #ClimateAction  #climatestrike   \n",
       "2                                     #ClimateStrike  #climatestrike   \n",
       "3  #greta #gretathunberg #climatechange #fridaysf...  #climatestrike   \n",
       "4  #greta #gretathunberg #climatechange #fridaysf...  #climatestrike   \n",
       "\n",
       "          location sentiment1  \n",
       "0  California, USA   negative  \n",
       "1  California, USA   positive  \n",
       "2  California, USA   positive  \n",
       "3  California, USA   positive  \n",
       "4  California, USA   positive  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seleccionar columnas de interes para la mineria de texto\n",
    "df_procesado = df[['text','date','hashtags','search_hashtags',\t'location',\t'sentiment1']]\n",
    "\n",
    "#eliminar registros con mas de 4 columnas en nulo\n",
    "df_procesado = df_procesado.dropna(thresh= 4 , axis=0 )\n",
    "\n",
    "# Convertir la columna \"date\" a formato str aaaa-mm-dd sin la hora\n",
    "df_procesado.loc[:,'date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#eliminar registros repetidos en los campos texto, fecha y ubicacion\n",
    "df_procesado = df_procesado.drop_duplicates(subset=['text','date','location'])\n",
    "\n",
    "#reiniciar indice de filas\n",
    "df_procesado.reset_index(drop= True, inplace=True)\n",
    "\n",
    "df_procesado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d140b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53399 entries, 0 to 53398\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   text             53399 non-null  object\n",
      " 1   date             53399 non-null  object\n",
      " 2   hashtags         53397 non-null  object\n",
      " 3   search_hashtags  53399 non-null  object\n",
      " 4   location         53399 non-null  object\n",
      " 5   sentiment1       21795 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_procesado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93bfe7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53399 entries, 0 to 53398\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               53399 non-null  int64 \n",
      " 1   text             53399 non-null  object\n",
      " 2   date             53399 non-null  object\n",
      " 3   hashtags         53399 non-null  object\n",
      " 4   search_hashtags  53399 non-null  object\n",
      " 5   location         53399 non-null  object\n",
      " 6   sentiment1       53399 non-null  object\n",
      " 7   aumented_text    53399 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Manejar NaN en la columna 'hashtags' y 'sentiment1'\n",
    "df_procesado.loc[:,'hashtags'] = df_procesado['hashtags'].fillna('')\n",
    "\n",
    "df_procesado.loc[:,'sentiment1'] = df_procesado['sentiment1'].fillna('')\n",
    "\n",
    "# unir columnas\n",
    "df_procesado.loc[:,'aumented_text'] = df_procesado['text'] + '. date: ' + df_procesado['date'] + '. location: ' + df_procesado['location'] + '. sentiment: ' + df_procesado['sentiment1']\n",
    "\n",
    "#crear indice para los embeddigs del texto de cada fila\n",
    "df_procesado = df_procesado.reset_index(names=\"id\")\n",
    "\n",
    "df_procesado.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cbc8b",
   "metadata": {},
   "source": [
    "### limpieza de texto columna aumentet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9665b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto:str):\n",
    "    #eliminar espacios en blanco, saltos de linea y pasar todo a minusculas\n",
    "    texto = texto.lower() \n",
    "    texto = re.sub(r'\\s+',  ' ', texto).strip()\n",
    "    texto = texto.replace(\"\\n\", \"\")\n",
    "    texto = texto.strip()\n",
    "    return texto\n",
    "\n",
    "df_procesado['clean_text']= df_procesado[\"aumented_text\"].apply(lambda x : limpiar_texto(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a3983",
   "metadata": {},
   "source": [
    "Segun la documentacion del modelo de embed text-embedding-ada-002 el numero de tokens no debe exceder 8192 tokens. Sino se debe dividir el texto. Para este dataset no es necesario, ningun texto supera ese numero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ac89bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# calcular el numero de tokest en el texto \n",
    "df_procesado['n_tokens_text'] = df_procesado[\"aumented_text\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df_procesado['n_tokens_clean'] = df_procesado[\"clean_text\"].apply(lambda x: len(tokenizer.encode(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c120323b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_text</th>\n",
       "      <th>n_tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53399.000000</td>\n",
       "      <td>53399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>67.865915</td>\n",
       "      <td>67.610405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.057471</td>\n",
       "      <td>24.884816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>455.000000</td>\n",
       "      <td>408.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_text  n_tokens_clean\n",
       "count   53399.000000    53399.000000\n",
       "mean       67.865915       67.610405\n",
       "std        25.057471       24.884816\n",
       "min        21.000000       21.000000\n",
       "25%        49.000000       49.000000\n",
       "50%        67.000000       67.000000\n",
       "75%        85.000000       84.000000\n",
       "max       455.000000      408.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_procesado[['n_tokens_text','n_tokens_clean']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03b29810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_metadata(row):\n",
    "    metadata = {\n",
    "        \"search_hashtags\": row[\"search_hashtags\"],\n",
    "        \"date\": row[\"date\"],\n",
    "        \"location\": row[\"location\"],\n",
    "        \"hashtags\": row[\"hashtags\"],\n",
    "        \"sentiment1\": row[\"sentiment1\"],\n",
    "        \"text\": row[\"clean_text\"]\n",
    "    }\n",
    "    return metadata\n",
    "df_procesado['metadata'] = df_procesado.apply(lambda row: crear_metadata(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9301cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_procesado = df_procesado[['id','clean_text','metadata']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2bbf6678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53399 entries, 0 to 53398\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          53399 non-null  int64 \n",
      " 1   clean_text  53399 non-null  object\n",
      " 2   metadata    53399 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final_procesado.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff3744",
   "metadata": {},
   "source": [
    "##   Crear embeddings con modelo de embeddings pre-entrenado de OpenIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa442d",
   "metadata": {},
   "source": [
    "### crear cliente para conexion con modelo de embedding de azure openIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92005fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = OPENAI_API_KEY,  \n",
    "  api_version = OPENAI_API_VERSION,\n",
    "  azure_endpoint = OPENAI_ENDPOINT\n",
    ")\n",
    "def generar_embedding(texto:str, modelo:str=EMBEDDING_DEPLOYMENT)->list[float]:\n",
    "    \"\"\"Funcion para crear embedings a partir de un texto usando modelo model \"\"\"\n",
    "    return client.embeddings.create(input = [texto], model=modelo).data[0].embedding\n",
    "\n",
    "def generar_embeddings_por_lote(textos:list, modelo:str=EMBEDDING_DEPLOYMENT)->list:\n",
    "    \"\"\"Funcion para crear embedings a partir de una lista de textos usando modelo model \"\"\"\n",
    "    return client.embeddings.create(input = textos, model=modelo).data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93205fc4",
   "metadata": {},
   "source": [
    "### Crear funcion para crear embeddings de textos en dataframe por lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "825dd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_df_por_lotes(df:pd.DataFrame,tamaño_lote:int= 200, guardar:bool = True)->pd.DataFrame:\n",
    "    \"\"\"Funcion para procesar dataframe por lotes de n_lote \"\"\"\n",
    "\n",
    "    # Dividir el DataFrame en lotes y procesar cada lote\n",
    "    lotes = []\n",
    "    lote_inicial =0\n",
    "    lote_actual = 0\n",
    "    for i in tqdm(range(0, len(df), tamaño_lote)):\n",
    "        # indice final del lote\n",
    "        i_end = min(len(df), i+tamaño_lote)\n",
    "        meta_batch = df[i:i_end]\n",
    "        # lista ids\n",
    "        ids_batch = [x['id'] for _,x in meta_batch.iterrows()]\n",
    "        # lista de textos para embed\n",
    "        textos = [x['clean_text'] for _,x in meta_batch.iterrows()]\n",
    "        # lista metadatos\n",
    "        # crear embeddings\n",
    "        try:\n",
    "            res = generar_embeddings_por_lote(textos, modelo=EMBEDDING_DEPLOYMENT)\n",
    "        except Exception as e:\n",
    "            done = False\n",
    "            print(f\"Reitentar solicitud, error {e}\")\n",
    "            while not done:\n",
    "                sleep(5)\n",
    "                try:\n",
    "                    res = generar_embeddings_por_lote(textos, modelo=EMBEDDING_DEPLOYMENT)\n",
    "                    done = True\n",
    "                except:\n",
    "                    print(f\"Reitentar solicitud, error {e}\")\n",
    "                    pass\n",
    "        embeds = [record.embedding for record in res]\n",
    "        metadata_batch = meta_batch['metadata'].to_list()\n",
    "\n",
    "        embeded_data = list(zip(ids_batch, textos ,embeds, metadata_batch))\n",
    "\n",
    "        # Crear un DataFrame a partir de los datos combinados\n",
    "        df_embeded = pd.DataFrame(embeded_data, columns=['id','text','embeddings','metadata'])\n",
    "\n",
    "        lotes.append(df_embeded)\n",
    "        lote_actual = lote_actual + 1\n",
    "        if (lote_actual % 20 == 0) and guardar:\n",
    "            \n",
    "            # Guardar el archivo temporal cada 2000 ejecuciones\n",
    "            file_name = f'climateTwitterEmbedData_{i_end}.csv'\n",
    "            file_path = os.path.join(src_path, data_path, 'out', 'batch', file_name)\n",
    "            lista_temp= lotes[lote_inicial:lote_actual]\n",
    "            print(f\"lote_actual->{lote_actual}\")\n",
    "            print(f\"lote_inicial->{lote_inicial}\")\n",
    "            print(f\"lista_temporal_guardada->{len(lista_temp)}\")\n",
    "\n",
    "\n",
    "            df_embeded_accum = pd.concat(lista_temp, ignore_index=True)\n",
    "            df_embeded_accum.to_csv(file_path, sep=\";\",index=False)\n",
    "            lote_inicial = lote_actual\n",
    "            \n",
    "  \n",
    "    # Concatenar los resultados en un solo DataFrame\n",
    "    df_out = pd.concat(lotes, ignore_index=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b3a69",
   "metadata": {},
   "source": [
    "### Crear df con ids, vectores y metadata para cargar en base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61706d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/267 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 19/267 [18:21<4:12:15, 61.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->20\n",
      "lote_inicial->0\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 39/267 [38:42<3:51:47, 61.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->40\n",
      "lote_inicial->20\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 59/267 [59:02<3:31:37, 61.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->60\n",
      "lote_inicial->40\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 79/267 [1:19:23<3:11:02, 60.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->80\n",
      "lote_inicial->60\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 99/267 [1:39:45<2:49:59, 60.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->100\n",
      "lote_inicial->80\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 119/267 [2:00:05<2:30:36, 61.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->120\n",
      "lote_inicial->100\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 139/267 [2:20:29<2:10:18, 61.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->140\n",
      "lote_inicial->120\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 159/267 [2:40:51<1:49:51, 61.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->160\n",
      "lote_inicial->140\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 179/267 [3:01:15<1:29:35, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->180\n",
      "lote_inicial->160\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 199/267 [3:21:37<1:09:07, 60.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->200\n",
      "lote_inicial->180\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 219/267 [3:42:00<48:52, 61.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->220\n",
      "lote_inicial->200\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 239/267 [4:02:24<28:38, 61.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->240\n",
      "lote_inicial->220\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 259/267 [4:22:45<08:05, 60.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lote_actual->260\n",
      "lote_inicial->240\n",
      "lista_temporal_guardada->20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [4:30:56<00:00, 60.89s/it]\n"
     ]
    }
   ],
   "source": [
    "df_embeded = procesar_df_por_lotes(df_final_procesado,tamaño_lote= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15f139a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020 is the year we #votethemout, the year we ...</td>\n",
       "      <td>[-0.0329987034201622, -0.04772832244634628, -0...</td>\n",
       "      <td>{'search_hashtags': '#climatestrike', 'date': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>winter has not stopped this group of dedicated...</td>\n",
       "      <td>[-0.022893592715263367, -0.0438067689538002, -...</td>\n",
       "      <td>{'search_hashtags': '#climatestrike', 'date': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>week 55 of #climatestrike at the @un. next wee...</td>\n",
       "      <td>[-0.02628657966852188, -0.038114212453365326, ...</td>\n",
       "      <td>{'search_hashtags': '#climatestrike', 'date': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a year of resistance, as youth protests shaped...</td>\n",
       "      <td>[-0.013956073671579361, -0.04684501141309738, ...</td>\n",
       "      <td>{'search_hashtags': '#climatestrike', 'date': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>happy holidays #greta #gretathunberg #climatec...</td>\n",
       "      <td>[-0.023563934490084648, -0.03334088623523712, ...</td>\n",
       "      <td>{'search_hashtags': '#climatestrike', 'date': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53394</th>\n",
       "      <td>53394</td>\n",
       "      <td>#endplasticwaste #savetheplanet can we just st...</td>\n",
       "      <td>[-0.02566179819405079, -0.012251610867679119, ...</td>\n",
       "      <td>{'search_hashtags': '#savetheplanet', 'date': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53395</th>\n",
       "      <td>53395</td>\n",
       "      <td>always feared this. #recycling #savetheplanet ...</td>\n",
       "      <td>[-0.010699223726987839, -0.015921304002404213,...</td>\n",
       "      <td>{'search_hashtags': '#savetheplanet', 'date': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53396</th>\n",
       "      <td>53396</td>\n",
       "      <td>no more straws at lbm... only if you ask for i...</td>\n",
       "      <td>[-0.03115925006568432, -0.022100692614912987, ...</td>\n",
       "      <td>{'search_hashtags': '#savetheplanet', 'date': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53397</th>\n",
       "      <td>53397</td>\n",
       "      <td>my #trumps may not believe in #climatechange b...</td>\n",
       "      <td>[-0.045478325337171555, -0.015456773340702057,...</td>\n",
       "      <td>{'search_hashtags': '#savetheplanet', 'date': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53398</th>\n",
       "      <td>53398</td>\n",
       "      <td>this is my first contribution on visualizing t...</td>\n",
       "      <td>[-0.012142631225287914, -0.01356309000402689, ...</td>\n",
       "      <td>{'search_hashtags': '#savetheplanet', 'date': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53399 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  \\\n",
       "0          0  2020 is the year we #votethemout, the year we ...   \n",
       "1          1  winter has not stopped this group of dedicated...   \n",
       "2          2  week 55 of #climatestrike at the @un. next wee...   \n",
       "3          3  a year of resistance, as youth protests shaped...   \n",
       "4          4  happy holidays #greta #gretathunberg #climatec...   \n",
       "...      ...                                                ...   \n",
       "53394  53394  #endplasticwaste #savetheplanet can we just st...   \n",
       "53395  53395  always feared this. #recycling #savetheplanet ...   \n",
       "53396  53396  no more straws at lbm... only if you ask for i...   \n",
       "53397  53397  my #trumps may not believe in #climatechange b...   \n",
       "53398  53398  this is my first contribution on visualizing t...   \n",
       "\n",
       "                                              embeddings  \\\n",
       "0      [-0.0329987034201622, -0.04772832244634628, -0...   \n",
       "1      [-0.022893592715263367, -0.0438067689538002, -...   \n",
       "2      [-0.02628657966852188, -0.038114212453365326, ...   \n",
       "3      [-0.013956073671579361, -0.04684501141309738, ...   \n",
       "4      [-0.023563934490084648, -0.03334088623523712, ...   \n",
       "...                                                  ...   \n",
       "53394  [-0.02566179819405079, -0.012251610867679119, ...   \n",
       "53395  [-0.010699223726987839, -0.015921304002404213,...   \n",
       "53396  [-0.03115925006568432, -0.022100692614912987, ...   \n",
       "53397  [-0.045478325337171555, -0.015456773340702057,...   \n",
       "53398  [-0.012142631225287914, -0.01356309000402689, ...   \n",
       "\n",
       "                                                metadata  \n",
       "0      {'search_hashtags': '#climatestrike', 'date': ...  \n",
       "1      {'search_hashtags': '#climatestrike', 'date': ...  \n",
       "2      {'search_hashtags': '#climatestrike', 'date': ...  \n",
       "3      {'search_hashtags': '#climatestrike', 'date': ...  \n",
       "4      {'search_hashtags': '#climatestrike', 'date': ...  \n",
       "...                                                  ...  \n",
       "53394  {'search_hashtags': '#savetheplanet', 'date': ...  \n",
       "53395  {'search_hashtags': '#savetheplanet', 'date': ...  \n",
       "53396  {'search_hashtags': '#savetheplanet', 'date': ...  \n",
       "53397  {'search_hashtags': '#savetheplanet', 'date': ...  \n",
       "53398  {'search_hashtags': '#savetheplanet', 'date': ...  \n",
       "\n",
       "[53399 rows x 4 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63bfc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en formato pkl\n",
    "file_name = \"climateTwitterEmbedData.pkl\"\n",
    "file_path = os.path.join(src_path, data_path, 'out', file_name)\n",
    "df_embeded.to_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d197214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\felip\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 27/27 [00:04<00:00,  6.40ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 27/27 [00:02<00:00,  9.22ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [02:15<00:00, 67.95s/it]\n",
      "c:\\Users\\felip\\Desktop\\2024\\MAESTRIA\\almacnamiento_y_recuperacion_informacion\\trabajo_2\\personal-pinecone-vdb\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\felip\\.cache\\huggingface\\hub\\datasets--AndresR2909--climate_twitter_text_embeddings. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AndresR2909/climate_twitter_text_embeddings/commit/f72264763ef619c587ab6dff286ad0ffbb5441a0', commit_message='Upload dataset', commit_description='', oid='f72264763ef619c587ab6dff286ad0ffbb5441a0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cargar df into  Hugging Face dataset\n",
    "login(token = access_token_hf, add_to_git_credential=False,write_permission= True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_embeded)\n",
    "dataset.push_to_hub(\"AndresR2909/climate_twitter_text_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8696f6",
   "metadata": {},
   "source": [
    "![image.png](hf_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd3728",
   "metadata": {},
   "source": [
    "## Prueba de busqueda de documentos localmente, sin base de datos vectorial, sobre el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c4972bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>metadata</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6365</th>\n",
       "      <td>6365</td>\n",
       "      <td>it appears like winter has been cancelled for ...</td>\n",
       "      <td>[-0.02680104970932007, -0.031904712319374084, ...</td>\n",
       "      <td>{'search_hashtags': '#globalwarming', 'date': ...</td>\n",
       "      <td>0.848808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51997</th>\n",
       "      <td>51997</td>\n",
       "      <td>it appears like winter has been cancelled for ...</td>\n",
       "      <td>[-0.027197187766432762, -0.029225224629044533,...</td>\n",
       "      <td>{'search_hashtags': '#globalwarming', 'date': ...</td>\n",
       "      <td>0.831487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>winter has not stopped this group of dedicated...</td>\n",
       "      <td>[-0.022893592715263367, -0.0438067689538002, -...</td>\n",
       "      <td>{'search_hashtags': '#climatestrike', 'date': ...</td>\n",
       "      <td>0.830322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32921</th>\n",
       "      <td>32921</td>\n",
       "      <td>winter is not coming #globalwarming . date: 20...</td>\n",
       "      <td>[-0.017101731151342392, -0.023731127381324768,...</td>\n",
       "      <td>{'search_hashtags': '#globalwarming', 'date': ...</td>\n",
       "      <td>0.824444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>1121</td>\n",
       "      <td>@usatodayweather it not normally this warm out...</td>\n",
       "      <td>[-0.0009998481255024672, -0.020653579384088516...</td>\n",
       "      <td>{'search_hashtags': '#climatestrike', 'date': ...</td>\n",
       "      <td>0.823834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  \\\n",
       "6365    6365  it appears like winter has been cancelled for ...   \n",
       "51997  51997  it appears like winter has been cancelled for ...   \n",
       "1          1  winter has not stopped this group of dedicated...   \n",
       "32921  32921  winter is not coming #globalwarming . date: 20...   \n",
       "1121    1121  @usatodayweather it not normally this warm out...   \n",
       "\n",
       "                                              embeddings  \\\n",
       "6365   [-0.02680104970932007, -0.031904712319374084, ...   \n",
       "51997  [-0.027197187766432762, -0.029225224629044533,...   \n",
       "1      [-0.022893592715263367, -0.0438067689538002, -...   \n",
       "32921  [-0.017101731151342392, -0.023731127381324768,...   \n",
       "1121   [-0.0009998481255024672, -0.020653579384088516...   \n",
       "\n",
       "                                                metadata  similarities  \n",
       "6365   {'search_hashtags': '#globalwarming', 'date': ...      0.848808  \n",
       "51997  {'search_hashtags': '#globalwarming', 'date': ...      0.831487  \n",
       "1      {'search_hashtags': '#climatestrike', 'date': ...      0.830322  \n",
       "32921  {'search_hashtags': '#globalwarming', 'date': ...      0.824444  \n",
       "1121   {'search_hashtags': '#climatestrike', 'date': ...      0.823834  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def similaridad_coseno(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def busqueda_documentos_dataframe(df: pd.DataFrame, user_query:str, top_n:int=4)->pd.DataFrame:\n",
    "    \"\"\"Funcion para buscar textos en dataframe\"\"\"\n",
    "    embedding = generar_embedding(\n",
    "        user_query,\n",
    "        modelo=\"text-embedding-ada-002\" \n",
    "    )\n",
    "    df[\"similarities\"] = df.embeddings.apply(lambda x: similaridad_coseno(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    display(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "res = busqueda_documentos_dataframe(df_embeded, \"why winter has not stopped in California in 2019-12\", top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b74f2ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_hashtags': '#globalwarming',\n",
       " 'date': '2019-12-30',\n",
       " 'location': 'California, USA',\n",
       " 'hashtags': '#GlobalWarming',\n",
       " 'sentiment1': 'positive',\n",
       " 'text': 'it appears like winter has been cancelled for maryland #globalwarming. date: 2019-12-30. location: california, usa. sentiment: positive'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.iloc[0][\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d8dfff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_hashtags': '#climatestrike',\n",
       " 'date': '2019-12-27',\n",
       " 'location': 'California, USA',\n",
       " 'hashtags': '#climatefriday #climatestrike #ClimateAction',\n",
       " 'sentiment1': 'positive',\n",
       " 'text': 'winter has not stopped this group of dedicated climate activists. they are an example to follow. #climatefriday #climatestrike #climateaction. date: 2019-12-27. location: california, usa. sentiment: positive'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.iloc[2][\"metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978c542",
   "metadata": {},
   "source": [
    "## Cargar embeddings a base de datos vectorial pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0539ad",
   "metadata": {},
   "source": [
    "### Iniciar conexion con base de datos vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d468d",
   "metadata": {},
   "source": [
    "\n",
    "#### seleccionar tipo de pod: Serverless or Pod-based\n",
    "Decidir que pod usar, ver documentacion: https://docs.pinecone.io/guides/indexes/configure-pod-based-indexes#changing-pod-sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a9512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize connection to pinecone (get API key at app.pc.io)\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42596a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pinecone import ServerlessSpec, PodSpec\n",
    "import time\n",
    "\n",
    "if use_serverless:\n",
    "    spec = ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "else:\n",
    "    spec = PodSpec(environment=environment,pod_type=\"s1.x1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e47a5",
   "metadata": {},
   "source": [
    "### Crear un indice\n",
    "\n",
    "crear un indice en  la vdb pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "201d1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'climate-twitter-data'\n",
    "\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# we create a new index\n",
    "pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
    "        metric='cosine', #'dotproduct'\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "# wait for index to be initialized\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b581fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'climate-twitter-data-s4apt1d.svc.gcp-starter.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'climate-twitter-data',\n",
       "              'spec': {'pod': {'environment': 'gcp-starter',\n",
       "                               'pod_type': 'starter',\n",
       "                               'pods': 1,\n",
       "                               'replicas': 1,\n",
       "                               'shards': 1}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirmar que fue creado el indice\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709f498",
   "metadata": {},
   "source": [
    "### conexion a indice de VDB creado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d33fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce1f869",
   "metadata": {},
   "source": [
    "### Cargar vectores al indice creado en base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc251e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sending upsert requests:   0%|          | 0/53399 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sending upsert requests: 100%|██████████| 53399/53399 [13:12<00:00, 67.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 53399}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.upsert_from_dataframe(df_embeded[['id','values','metadata']], batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbe9c902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.53399,\n",
       " 'namespaces': {'': {'vector_count': 53399}},\n",
       " 'total_vector_count': 53399}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651cbabf",
   "metadata": {},
   "source": [
    "![image.png](pinecone_vdb.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
